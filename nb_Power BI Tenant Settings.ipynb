{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6b725-31a4-43f1-95ec-ceba09f2ba6f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cf186-b353-4bfa-bfc9-14627c899701",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Example: if your files are in Files/utils/ within the default Lakehouse\n",
    "sys.path.append(\"/lakehouse/default/Files/utils/\")\n",
    "print(f\"Added {sys.path[-1]} to Python path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2077b-af43-49fd-8651-c172a093e667",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    from powerbi_api_utils import load_config, get_access_token\n",
    "    from fabric_utils import cast_dataframe_to_fabric_compatible_types\n",
    "    print(\"Helper scripts loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing helper scripts: {e}\")\n",
    "    print(\"Please ensure powerbi_api_utils.py and fabric_utils.py are in a 'utils' directory accessible by this notebook.\")\n",
    "    raise\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_fabric_api_constants(tenant_id):\n",
    "    \"\"\"Returns Microsoft Fabric API constants.\"\"\"\n",
    "    AUTHORITY = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "    # NOTE: The scope for Fabric APIs is different from Power BI APIs\n",
    "    SCOPE = [\"https://api.fabric.microsoft.com/.default\"]\n",
    "    ADMIN_BASE_URL = \"https://api.fabric.microsoft.com/v1/admin\"\n",
    "    return AUTHORITY, SCOPE, ADMIN_BASE_URL\n",
    "\n",
    "def main(spark: SparkSession):\n",
    "    \"\"\"Main function to orchestrate the extraction and loading process.\"\"\"\n",
    "    try:\n",
    "        # --- 2. Authentication ---\n",
    "        logging.info(\"Loading configuration...\")\n",
    "        config = load_config()\n",
    "        TENANT_ID = config[\"TENANT_ID\"]\n",
    "        \n",
    "        logging.info(\"Setting up Fabric API constants...\")\n",
    "        AUTHORITY, SCOPE, ADMIN_BASE_URL = get_fabric_api_constants(TENANT_ID)\n",
    "\n",
    "        logging.info(\"Acquiring access token for Fabric API...\")\n",
    "        access_token = get_access_token(\n",
    "            config[\"CLIENT_ID\"], config[\"CLIENT_SECRET\"], AUTHORITY, SCOPE\n",
    "        )\n",
    "        headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "        # --- 3. Data Fetching ---\n",
    "        tenant_settings_url = f\"{ADMIN_BASE_URL}/tenantsettings\"\n",
    "        logging.info(f\"Fetching data from endpoint: {tenant_settings_url}\")\n",
    "        \n",
    "        response = requests.get(tenant_settings_url, headers=headers, verify=config[\"VERIFY_SSL\"])\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        raw_data = response.json()\n",
    "        logging.info(\"Successfully fetched tenant settings data.\")\n",
    "\n",
    "        # --- 4. Data Normalization & Transformation ---\n",
    "        logging.info(\"Normalizing and transforming data...\")\n",
    "        \n",
    "        tenant_settings_list = raw_data.get('tenantSettings', [])\n",
    "        \n",
    "        if not tenant_settings_list:\n",
    "            logging.warning(\"No tenant settings found in the API response. Exiting.\")\n",
    "            return\n",
    "\n",
    "        all_security_groups = []\n",
    "        all_properties = []\n",
    "\n",
    "        # Create the main dataframe\n",
    "        df_tenant_settings = pd.DataFrame(tenant_settings_list)\n",
    "\n",
    "        # Use settingName as the foreign key to link child tables back to the parent\n",
    "        for index, row in df_tenant_settings.iterrows():\n",
    "            parent_setting_name = row['settingName']\n",
    "\n",
    "            # Extract Enabled Security Groups\n",
    "            security_groups = row.get('enabledSecurityGroups')\n",
    "            if isinstance(security_groups, list) and security_groups:\n",
    "                for group in security_groups:\n",
    "                    group['settingName'] = parent_setting_name  # Add foreign key\n",
    "                    all_security_groups.append(group)\n",
    "\n",
    "            # Extract Properties\n",
    "            properties = row.get('properties')\n",
    "            if isinstance(properties, list) and properties:\n",
    "                for prop in properties:\n",
    "                    prop['settingName'] = parent_setting_name  # Add foreign key\n",
    "                    all_properties.append(prop)\n",
    "\n",
    "        # Create child dataframes\n",
    "        df_security_groups = pd.DataFrame(all_security_groups)\n",
    "        df_properties = pd.DataFrame(all_properties)\n",
    "        \n",
    "        # Drop the original nested columns from the main dataframe\n",
    "        df_tenant_settings = df_tenant_settings.drop(columns=['enabledSecurityGroups', 'properties'], errors='ignore')\n",
    "\n",
    "        logging.info(f\"Created main table 'tenant_settings' with {len(df_tenant_settings)} rows.\")\n",
    "        logging.info(f\"Created child table 'enabled_security_groups' with {len(df_security_groups)} rows.\")\n",
    "        logging.info(f\"Created child table 'properties' with {len(df_properties)} rows.\")\n",
    "\n",
    "        # --- 5. Saving Data to Lakehouse (Overwrite Mode) ---\n",
    "        \n",
    "        dfs_to_save = {\n",
    "            \"tenant_settings\": df_tenant_settings,\n",
    "            \"enabled_security_groups\": df_security_groups,\n",
    "            \"properties\": df_properties\n",
    "        }\n",
    "        \n",
    "        warehouse_schema = \"powerbi_metadata\"\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {warehouse_schema}\")\n",
    "        logging.info(f\"Ensured schema '{warehouse_schema}' exists.\")\n",
    "        \n",
    "        for table_name, df_pandas in dfs_to_save.items():\n",
    "            full_table_name = f\"{warehouse_schema}.{table_name}\"\n",
    "            logging.info(f\"Processing and saving table '{full_table_name}'...\")\n",
    "\n",
    "            if df_pandas.empty:\n",
    "                logging.warning(f\"DataFrame for '{table_name}' is empty. Creating/overwriting with an empty table.\")\n",
    "                # Create an empty Spark DF with a dummy column to ensure the table is created/overwritten\n",
    "                spark.createDataFrame([], schema=\"dummy STRING\").drop(\"dummy\").write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "                continue\n",
    "\n",
    "            # Cast types to be compatible with Fabric Warehouse\n",
    "            df_casted = cast_dataframe_to_fabric_compatible_types(df_pandas)\n",
    "            \n",
    "            # Add extraction timestamp\n",
    "            df_casted['extraction_timestamp'] = pd.Timestamp.now()\n",
    "            \n",
    "            # Create Spark DataFrame and save with overwrite mode\n",
    "            spark_df = spark.createDataFrame(df_casted)\n",
    "            spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            \n",
    "            logging.info(f\"✅ Successfully saved data to {full_table_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ An error occurred during the process: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the SparkSession from the Fabric notebook environment\n",
    "    spark_session = globals().get('spark')\n",
    "    if spark_session:\n",
    "        main(spark_session)\n",
    "    else:\n",
    "        print(\"SparkSession not found. Please run this in a Fabric notebook environment.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "04562fe6-e41b-4e15-80c1-0585600c90d4",
    "default_lakehouse_name": "PowerBI_Metadata",
    "default_lakehouse_workspace_id": "c937daa0-0279-4f6d-baa3-652ba7a2c960",
    "known_lakehouses": [
     {
      "id": "04562fe6-e41b-4e15-80c1-0585600c90d4"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
