{"cells":[{"cell_type":"code","source":["import msal\n","import requests\n","import pandas as pd\n","import time\n","import json\n","import os\n","from datetime import datetime, timedelta, timezone \n","import urllib.parse\n","\n","# For Fabric Lakehouse operations\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import lit, current_timestamp, col\n","from pyspark.sql.types import StringType, IntegerType, BooleanType, TimestampType, StructType, StructField, MapType\n","\n","# For explicit Delta table operations\n","from delta.tables import DeltaTable \n","\n","try:\n","    spark = SparkSession.builder.getOrCreate()\n","    print(\"Spark session initialized.\")\n","except Exception as e:\n","    print(f\"Error initializing Spark session: {e}\")\n","    pass \n","\n","\n","# --- Configuration ---\n","CONFIG_FILE_PATH = \"/lakehouse/default/Files/utils/config.json\"\n","\n","# Lakehouse table name - This is the name your Delta table will have in the 'Tables' section\n","LAKEHOUSE_TABLE_NAME = \"activity_events\" \n","\n","# Load configuration from config.json\n","try:\n","    with open(CONFIG_FILE_PATH, 'r') as f:\n","        config = json.load(f)\n","    print(f\"Successfully loaded configuration from {CONFIG_FILE_PATH}\")\n","except FileNotFoundError:\n","    print(f\"Error: {CONFIG_FILE_PATH} not found. Please ensure the config.json file exists at the specified path.\")\n","    exit(1)\n","except json.JSONDecodeError:\n","    print(f\"Error: Could not decode JSON from {CONFIG_FILE_PATH}. Please check the file's format.\")\n","    exit(1)\n","except Exception as e:\n","    print(f\"An unexpected error occurred while loading config.json: {e}\")\n","    exit(1)\n","\n","# Get Power BI credentials from config.json.\n","CLIENT_ID = config.get(\"CLIENT_ID\", \"YOUR_APPLICATION_CLIENT_ID\")\n","CLIENT_SECRET = config.get(\"CLIENT_SECRET\", \"YOUR_CLIENT_SECRET\")\n","TENANT_ID = config.get(\"TENANT_ID\", \"YOUR_DIRECTORY_TENANT_ID\")\n","\n","# --- Security & Verification ---\n","# It's recommended to keep VERIFY_SSL = True in production.\n","VERIFY_SSL = False\n","\n","if not VERIFY_SSL:\n","    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n","    print(\"!!! WARNING: SSL CERTIFICATE VERIFICATION IS DISABLED.                     !!!\")\n","    print(\"!!! This is a security risk and should only be a temporary workaround.     !!!\")\n","    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n","    import urllib3\n","    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n","\n","# --- API Constants ---\n","AUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n","SCOPE = [\"https://analysis.windows.net/powerbi/api/.default\"]\n","BASE_URL = \"https://api.powerbi.com/v1.0/myorg\"\n","ADMIN_BASE_URL = f\"{BASE_URL}/admin\"\n","\n","# --- Authentication ---\n","def get_access_token():\n","    \"\"\"Authenticates using MSAL with client credentials and returns an access token.\"\"\"\n","    if \"YOUR_\" in CLIENT_ID or \"YOUR_\" in CLIENT_SECRET or \"YOUR_\" in TENANT_ID:\n","        raise ValueError(\n","            \"Placeholder Power BI credentials detected. Please set CLIENT_ID, \"\n","            \"CLIENT_SECRET, and TENANT_ID in your config.json file.\"\n","        )\n","\n","    app = msal.ConfidentialClientApplication(\n","        CLIENT_ID,\n","        authority=AUTHORITY,\n","        client_credential=CLIENT_SECRET\n","    )\n","    result = app.acquire_token_silent(SCOPE, account=None)\n","    if not result:\n","        print(\"No suitable token in cache, acquiring a new one...\")\n","        result = app.acquire_token_for_client(scopes=SCOPE)\n","\n","    if \"access_token\" in result:\n","        return result[\"access_token\"]\n","    else:\n","        print(\"Error acquiring token:\")\n","        print(result.get(\"error\"))\n","        print(result.get(\"error_description\"))\n","        raise Exception(\"Failed to acquire access token for Service Principal.\")\n","\n","# --- Data Fetching for Activity Events ---\n","def get_paginated_activity_data(token, start_date_time, end_date_time):\n","    \"\"\"\n","    Retrieves all activity events from the Power BI Admin API, handling\n","    pagination using 'continuationUri' and 'lastResultSet'.\n","    \"\"\"\n","    all_events = []\n","    headers = {\"Authorization\": f\"Bearer {token}\"}\n","    \n","    base_url_activity_events = f\"{ADMIN_BASE_URL}/activityevents\"\n","    \n","    initial_request_params = {\n","        'startDateTime': start_date_time, \n","        'endDateTime': end_date_time\n","    }\n","    \n","    next_page_uri = None\n","    is_first_request = True\n","\n","    while True:\n","        request_url = \"\"\n","        request_params = {}\n","\n","        if is_first_request:\n","            request_url = base_url_activity_events\n","            request_params = initial_request_params.copy()\n","            is_first_request = False\n","        elif next_page_uri:\n","            request_url = next_page_uri\n","            request_params = {}\n","        else:\n","            print(\"     > No more activity events or continuation URI found. Completing fetch.\")\n","            break \n","        \n","        try:\n","            print(f\"Fetching activity events from: {request_url}\")\n","            if request_params:\n","                print(f\"     > With params: {request_params}\")\n","\n","            response = requests.get(request_url, headers=headers, params=request_params, verify=VERIFY_SSL)\n","            response.raise_for_status()\n","            data = response.json()\n","\n","            page_events = data.get(\"activityEventEntities\", [])\n","            all_events.extend(page_events)\n","            print(f\"     > Retrieved {len(page_events)} items. Total so far: {len(all_events)}.\")\n","\n","            next_page_uri = data.get(\"continuationUri\")\n","            last_result_set = data.get(\"lastResultSet\", False)\n","\n","            if last_result_set:\n","                print(\"     > Reached the last result set. Completing fetch.\")\n","                break\n","\n","        except requests.exceptions.HTTPError as e:\n","            if e.response.status_code == 429:\n","                retry_after = int(e.response.headers.get(\"Retry-After\", 30))\n","                print(f\"     > Rate limited. Retrying after {retry_after} seconds...\")\n","                time.sleep(retry_after)\n","            else:\n","                print(f\"HTTP Error fetching activity events: {e.response.status_code} - {e.response.text}\")\n","                break\n","        except Exception as e:\n","            print(f\"An unexpected error occurred while fetching activity events: {e}\")\n","            break\n","        \n","        time.sleep(1)\n","\n","    return all_events\n","\n","# --- Fabric Lakehouse Operations ---\n","def write_activity_events_to_lakehouse(activity_events, bDate_str):\n","    \"\"\"\n","    Converts activity events to a Spark DataFrame and writes/merges them\n","    into the specified Fabric Lakehouse table using explicit delete and append.\n","    \"\"\"\n","    if not activity_events:\n","        print(\"No activity events to write to Lakehouse.\")\n","        return\n","\n","    # Define the schema explicitly for the DataFrame to ensure correct types\n","    schema = StructType([\n","        StructField(\"Id\", StringType(), True),\n","        StructField(\"RecordType\", IntegerType(), True),\n","        StructField(\"CreationTime\", StringType(), True), \n","        StructField(\"Operation\", StringType(), True),\n","        StructField(\"OrganizationId\", StringType(), True),\n","        StructField(\"UserType\", StringType(), True),\n","        StructField(\"UserKey\", StringType(), True),\n","        StructField(\"Workload\", StringType(), True),\n","        StructField(\"UserId\", StringType(), True),\n","        StructField(\"Activity\", StringType(), True),\n","        StructField(\"EmbedTokenId\", StringType(), True),\n","        StructField(\"IsSuccess\", BooleanType(), True),\n","        StructField(\"RequestId\", StringType(), True),\n","        StructField(\"ActivityId\", StringType(), True),\n","        StructField(\"BillingType\", StringType(), True),\n","        StructField(\"ClientIP\", StringType(), True),\n","        StructField(\"UserAgent\", StringType(), True),\n","        StructField(\"ItemName\", StringType(), True),\n","        StructField(\"WorkSpaceName\", StringType(), True),\n","        StructField(\"CapacityId\", StringType(), True),\n","        StructField(\"CapacityName\", StringType(), True),\n","        StructField(\"WorkspaceId\", StringType(), True),\n","        StructField(\"ObjectId\", StringType(), True),\n","        StructField(\"DataflowId\", StringType(), True),\n","        StructField(\"DataflowName\", StringType(), True),\n","        StructField(\"DataflowType\", StringType(), True),\n","        StructField(\"DatasetName\", StringType(), True),\n","        StructField(\"DatasetId\", StringType(), True),\n","        StructField(\"DataConnectivityMode\", StringType(), True),\n","        StructField(\"ArtifactId\", StringType(), True),\n","        StructField(\"ArtifactName\", StringType(), True),\n","        StructField(\"RefreshType\", StringType(), True),\n","        StructField(\"LastRefreshTime\", StringType(), True),\n","        StructField(\"ArtifactKind\", StringType(), True),\n","        StructField(\"ItemId\", StringType(), True),\n","        StructField(\"ReportName\", StringType(), True),\n","        StructField(\"AppName\", StringType(), True),\n","        StructField(\"ReportId\", StringType(), True),\n","        StructField(\"ReportType\", StringType(), True),\n","        StructField(\"AppReportId\", StringType(), True),\n","        StructField(\"DistributionMethod\", StringType(), True),\n","        StructField(\"ConsumptionMethod\", StringType(), True),\n","        StructField(\"AppId\", StringType(), True),\n","        StructField(\"DataflowRefreshScheduleType\", StringType(), True),\n","        StructField(\"ExportedArtifactInfo\", StringType(), True),\n","        StructField(\"ExportedArtifactDownloadInfo\", StringType(), True),\n","        StructField(\"EndPoint\", StringType(), True),\n","        StructField(\"HasFullReportAttachment\", StringType(), True), \n","        StructField(\"SubscriptionDetails\", StringType(), True),\n","        StructField(\"GatewayId\", StringType(), True),\n","        StructField(\"DatasourceId\", StringType(), True),\n","        StructField(\"SubfolderId\", StringType(), True),\n","        StructField(\"SubfolderObjectId\", StringType(), True),\n","        StructField(\"SubfolderName\", StringType(), True),\n","        StructField(\"FolderObjectId\", StringType(), True),\n","        StructField(\"FolderDisplayName\", StringType(), True),\n","        StructField(\"FolderAccessRequests\", StringType(), True),\n","        StructField(\"TableName\", StringType(), True),\n","        StructField(\"ArtifactAccessRequestInfo\", StringType(), True),\n","        StructField(\"Schedules\", StringType(), True),\n","        StructField(\"OriginalOwner\", StringType(), True),\n","        StructField(\"TakingOverOwner\", StringType(), True),\n","        StructField(\"SubscribeeInformation\", StringType(), True),\n","        StructField(\"ExternalSubscribeeInformation\", StringType(), True),\n","        StructField(\"SubscriptionSchedule\", StringType(), True),\n","        StructField(\"IsTenantAdminApi\", StringType(), True), \n","        StructField(\"GatewayClustersObjectIds\", StringType(), True),\n","        StructField(\"DatasourceInformations\", StringType(), True),\n","        StructField(\"ArtifactObjectId\", StringType(), True),\n","        StructField(\"AdditionalInfo\", StringType(), True) # Store as JSON string\n","    ])\n","\n","    # Transform list of dictionaries into a list of tuples/lists that match the schema\n","    data_for_df = []\n","    explicit_keys = [field.name for field in schema.fields if field.name != \"AdditionalInfo\"]\n","\n","    for event in activity_events:\n","        row = []\n","        additional_info_dict = {}\n","        for key in explicit_keys:\n","            value = event.get(key)\n","            if key == \"IsSuccess\":\n","                row.append(bool(value)) \n","            elif key in [\"HasFullReportAttachment\", \"IsTenantAdminApi\"] and isinstance(value, str):\n","                row.append(value.lower() == 'true' if value else None)\n","            else:\n","                row.append(value)\n","        \n","        for k, v in event.items():\n","            if k not in explicit_keys:\n","                if isinstance(v, (list, dict)):\n","                    try:\n","                        additional_info_dict[k] = json.dumps(v)\n","                    except TypeError:\n","                        additional_info_dict[k] = str(v)\n","                else:\n","                    additional_info_dict[k] = v\n","        \n","        row.append(json.dumps(additional_info_dict)) \n","        data_for_df.append(row)\n","\n","    # Create Spark DataFrame\n","    df = spark.createDataFrame(data_for_df, schema=schema)\n","    \n","    # Add a 'LoadDate' column for tracking when the data was loaded.\n","    df = df.withColumn(\"LoadDate\", lit(bDate_str)) \n","    df = df.withColumn(\"ProcessingTimestamp\", current_timestamp()) \n","\n","    print(f\"DataFrame created with {df.count()} rows and schema:\")\n","    df.printSchema()\n","\n","    # --- Explicit Delete and Append Logic ---\n","    try:\n","        # Check if the table already exists\n","        if spark.catalog.tableExists(LAKEHOUSE_TABLE_NAME):\n","            print(f\"\\nDelta table '{LAKEHOUSE_TABLE_NAME}' exists. Proceeding with delete and append.\")\n","            delta_table = DeltaTable.forName(spark, LAKEHOUSE_TABLE_NAME)\n","\n","            # Delete existing records for the current LoadDate\n","            print(f\"Attempting to delete existing records for LoadDate = '{bDate_str}'...\")\n","            delete_condition = f\"LoadDate = '{bDate_str}'\"\n","            delta_table.delete(delete_condition)\n","            print(f\"Successfully deleted existing records for LoadDate = '{bDate_str}'.\")\n","\n","            # Append new data\n","            df.write \\\n","              .format(\"delta\") \\\n","              .mode(\"append\") \\\n","              .partitionBy(\"LoadDate\") \\\n","              .saveAsTable(LAKEHOUSE_TABLE_NAME)\n","            print(f\"\\nSuccessfully appended new Power BI activity events for {bDate_str} to Lakehouse table: {LAKEHOUSE_TABLE_NAME}\")\n","\n","        else:\n","            # If the table doesn't exist, create it with the first set of data\n","            print(f\"\\nDelta table '{LAKEHOUSE_TABLE_NAME}' does not exist. Creating it now.\")\n","            df.write \\\n","              .format(\"delta\") \\\n","              .mode(\"append\") \\\n","              .option(\"overwriteSchema\", \"true\") \\\n","              .partitionBy(\"LoadDate\") \\\n","              .saveAsTable(LAKEHOUSE_TABLE_NAME)\n","            print(f\"\\nSuccessfully created and written Power BI activity events for {bDate_str} to Lakehouse table: {LAKEHOUSE_TABLE_NAME}\")\n","\n","    except Exception as e:\n","        print(f\"Error writing to Lakehouse: {e}\")\n","        print(\"Please ensure your Fabric workspace and lakehouse are correctly configured and you have write permissions.\")\n","        print(\"Also, check if the schema matches and that you have a 'LoadDate' column for partitioning.\")\n","\n","\n","# --- Main Execution ---\n","def main(date_to_process: str = None):\n","    \"\"\"\n","    Main function to run the activity events extraction and Lakehouse insertion process.\n","    :param date_to_process: Optional date string in 'YYYY-MM-DD' format. If None,\n","                            defaults to the previous day (UTC).\n","    \"\"\"\n","    try:\n","        print(\"Attempting to authenticate Power BI with Service Principal...\")\n","        access_token = get_access_token()\n","        print(\"Successfully authenticated and acquired access token.\\n\")\n","\n","        if date_to_process:\n","            try:\n","                selected_date = datetime.strptime(date_to_process, \"%Y-%m-%d\").date()\n","                bDate_str = selected_date.strftime(\"%Y-%m-%d\")\n","                print(f\"Running for manually entered date: {bDate_str}\")\n","            except ValueError:\n","                print(f\"Invalid date format '{date_to_process}'. Expected YYYY-MM-DD. \"\n","                      \"Using previous day's date instead.\")\n","                previous_day_utc = datetime.now(timezone.utc).date() - timedelta(days=1)\n","                bDate_str = previous_day_utc.strftime(\"%Y-%m-%d\")\n","                print(f\"Running for previous day (UTC): {bDate_str}\")\n","        else:\n","            previous_day_utc = datetime.now(timezone.utc).date() - timedelta(days=1)\n","            bDate_str = previous_day_utc.strftime(\"%Y-%m-%d\")\n","            print(f\"No date provided. Running for previous day (UTC): {bDate_str}\")\n","        \n","        # Date-time format as required by the API (enclosed in single quotes, with .000Z)\n","        start_datetime_str = f\"'{bDate_str}T00:00:00.000Z'\" \n","        end_datetime_str = f\"'{bDate_str}T23:59:59.000Z'\"\n","        \n","        print(f\"\\n--- Fetching all activity events for {bDate_str} ---\")\n","        activity_events = get_paginated_activity_data(\n","            access_token,\n","            start_datetime_str,\n","            end_datetime_str\n","        )\n","\n","        if activity_events:\n","            write_activity_events_to_lakehouse(activity_events, bDate_str)\n","        else:\n","            print(f\"No activity events found for {bDate_str}. Skipping Lakehouse insertion.\")\n","\n","    except Exception as e:\n","        print(f\"\\nAn error occurred during the process: {e}\")\n","        print(\"Please check your configuration, permissions, and network connectivity.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ad22e27-059a-4d2c-b4ba-0a2e36422f1b"},{"cell_type":"code","source":["main() "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":61,"statement_ids":[61],"state":"finished","livy_statement_state":"available","session_id":"a5d7601d-e37f-4496-80fb-8f613b9c4a79","normalized_state":"finished","queued_time":"2025-07-16T17:11:04.1777554Z","session_start_time":null,"execution_start_time":"2025-07-16T17:11:04.4316906Z","execution_finish_time":"2025-07-16T17:11:05.9431754Z","parent_msg_id":"0d9ea6a2-c44d-4a39-98ff-0e5a3435c63a"},"text/plain":"StatementMeta(, a5d7601d-e37f-4496-80fb-8f613b9c4a79, 61, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Attempting to authenticate Power BI with Service Principal...\nNo suitable token in cache, acquiring a new one...\nSuccessfully authenticated and acquired access token.\n\nRunning for manually entered date: 2025-06-18\n\n--- Fetching all activity events for 2025-06-18 ---\nFetching activity events from: https://api.powerbi.com/v1.0/myorg/admin/activityevents\n     > With params: {'startDateTime': \"'2025-06-18T00:00:00.000Z'\", 'endDateTime': \"'2025-06-18T23:59:59.000Z'\"}\nHTTP Error fetching activity events: 400 - \nNo activity events found for 2025-06-18. Skipping Lakehouse insertion.\n"]}],"execution_count":60,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f636284-f3c7-4afb-a70f-5603b2d466a7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"04562fe6-e41b-4e15-80c1-0585600c90d4","known_lakehouses":[{"id":"04562fe6-e41b-4e15-80c1-0585600c90d4"}],"default_lakehouse_name":"PowerBI_Metadata","default_lakehouse_workspace_id":"c937daa0-0279-4f6d-baa3-652ba7a2c960"}}},"nbformat":4,"nbformat_minor":5}