{"cells":[{"cell_type":"code","source":["pip install httpx"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ca1e1e09-9207-4d47-b6fb-794046428253"},{"cell_type":"code","source":["import sys\n","# Example: if your files are in Files/utils/ within the default Lakehouse\n","sys.path.append(\"/lakehouse/default/Files/utils/\")\n","print(f\"Added {sys.path[-1]} to Python path.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"12ad0eea-d215-440a-a558-e67bfd892bfa"},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import logging\n","import asyncio\n","import httpx\n","\n","from powerbi_api_utils import load_config, get_api_constants, get_access_token, get_paginated_data_async\n","from fabric_utils import save_to_fabric_warehouse\n","\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Standardizes DataFrame column names.\"\"\"\n","    if not df.empty:\n","        df.columns = [str(col).replace('.', '_').replace(' ', '_') for col in df.columns]\n","    return df\n","\n","def _normalize_and_merge_json_column(df: pd.DataFrame, col_name: str, prefix: str) -> pd.DataFrame:\n","    \"\"\"Expands a column containing JSON/dicts into new columns and merges back.\"\"\"\n","    if df.empty or col_name not in df.columns:\n","        return df\n","    df[col_name] = df[col_name].apply(lambda x: x if isinstance(x, dict) and x else None)\n","    to_normalize = df.dropna(subset=[col_name]).copy()\n","    if to_normalize.empty:\n","        return df.drop(columns=[col_name], errors='ignore')\n","    normalized_df = pd.json_normalize(to_normalize[col_name]).add_prefix(prefix)\n","    df = df.drop(columns=[col_name])\n","    df = df.merge(normalized_df, left_index=True, right_index=True, how='left')\n","    return _clean_columns(df)\n","\n","async def get_all_metadata_async(token: str, base_url: str, admin_base_url: str, verify_ssl: bool, include_report_app_users: bool = True):\n","    \"\"\"Orchestrates the fetching of all Power BI metadata concurrently.\"\"\"\n","    headers = {'Authorization': f'Bearer {token}'}\n","    \n","    async with httpx.AsyncClient(verify=verify_ssl) as client:\n","        # --- 1. Fetch Capacities ---\n","        logging.info(\"--- Fetching Tenant-Level Admin Data ---\")\n","        capacities_data = await get_paginated_data_async(client, f\"{admin_base_url}/capacities\", headers)\n","        capacities_df = _clean_columns(pd.DataFrame(capacities_data))\n","        if 'users' in capacities_df.columns:\n","            capacities_df = capacities_df.drop(columns=['users'])\n","        if not capacities_df.empty and 'admins' in capacities_df.columns:\n","            capacities_df['admins'] = capacities_df['admins'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n","\n","        # --- 2. Fetch Gateways ---\n","        logging.info(\"--- Fetching Gateways and their details ---\")\n","        gateways_data = await get_paginated_data_async(client, f\"{base_url}/gateways\", headers)\n","        \n","        gateway_tasks = [get_paginated_data_async(client, f\"{base_url}/gateways/{gw.get('id')}/datasources\", headers) for gw in gateways_data if gw.get('id')]\n","        all_datasources_results = await asyncio.gather(*gateway_tasks)\n","        \n","        all_gateway_datasources = []\n","        for gateway, datasources in zip(gateways_data, all_datasources_results):\n","            for ds in datasources:\n","                ds['gatewayId'] = gateway.get('id')\n","                all_gateway_datasources.append(ds)\n","        \n","        gateways_df = _clean_columns(pd.DataFrame(gateways_data)).drop(columns=['publicKey', 'gatewayAnnotation'], errors='ignore')\n","        \n","        # --- FIX: Correctly process gateway_datasources_df ---\n","        gateway_datasources_df = _clean_columns(pd.DataFrame(all_gateway_datasources))\n","        # First, flatten 'credentialDetails' as per the original logic\n","        gateway_datasources_df = _normalize_and_merge_json_column(gateway_datasources_df, 'credentialDetails', 'credentialDetails_')\n","        # Then, flatten 'details' if it exists\n","        if 'details' in gateway_datasources_df.columns:\n","            gateway_datasources_df = _normalize_and_merge_json_column(gateway_datasources_df, 'details', 'details_')\n","\n","        # --- Fetch Gateway Datasource Users ---\n","        datasource_user_tasks = [\n","            (get_paginated_data_async(client, f\"{base_url}/gateways/{ds.get('gatewayId')}/datasources/{ds.get('id')}/users\", headers), ds.get('gatewayId'), ds.get('id'))\n","            for ds in all_gateway_datasources if ds.get('id')\n","        ]\n","        user_task_futures = [t[0] for t in datasource_user_tasks]\n","        all_datasource_users_results = await asyncio.gather(*user_task_futures)\n","        all_datasource_users = []\n","        for (task, gateway_id, ds_id), users in zip(datasource_user_tasks, all_datasource_users_results):\n","            for user in users:\n","                user.update({'gatewayId': gateway_id, 'datasourceId': ds_id})\n","                all_datasource_users.append(user)\n","        gateway_datasource_users_df = _clean_columns(pd.DataFrame(all_datasource_users))\n","        if 'users' in gateway_datasource_users_df.columns:\n","            gateway_datasource_users_df = gateway_datasource_users_df.drop(columns=['users'])\n","            \n","        # --- 3. Fetch Workspaces and related assets ---\n","        logging.info(\"--- Fetching Workspace-Level Data ---\")\n","        workspaces_url = f\"{admin_base_url}/groups\"; params = {\"$expand\": \"users,reports,datasets,dataflows\", \"$filter\": \"type eq 'Workspace' and state eq 'Active'\", \"$top\": 5000}\n","        workspaces_data = await get_paginated_data_async(client, workspaces_url, headers, params=params)\n","        \n","        all_workspaces, all_workspace_users, all_reports_raw, all_datasets_raw, all_dataflows_raw = [], [], [], [], []\n","        for ws in workspaces_data:\n","            ws_id, ws_name = ws.get('id'), ws.get('name')\n","            all_workspaces.append({k: v for k, v in ws.items() if k in ['id', 'name', 'isOnDedicatedCapacity', 'capacityId', 'type', 'state']})\n","            for item_list, item_type in [(ws.get('users', []), all_workspace_users), (ws.get('reports', []), all_reports_raw), (ws.get('datasets', []), all_datasets_raw), (ws.get('dataflows', []), all_dataflows_raw)]:\n","                for item in item_list: item.update({'workspace_id': ws_id, 'workspace_name': ws_name}); item_type.append(item)\n","\n","        # --- ENFORCE SCHEMAS: Create and clean DataFrames ---\n","        workspaces_df = _clean_columns(pd.DataFrame(all_workspaces))\n","        workspace_users_df = _normalize_and_merge_json_column(_clean_columns(pd.DataFrame(all_workspace_users)), 'profile', 'profile_')\n","        reports_df = _clean_columns(pd.DataFrame(all_reports_raw)); reports_df = reports_df.drop(columns=['users', 'subscriptions'], errors='ignore')\n","        datasets_df = _clean_columns(pd.DataFrame(all_datasets_raw)); datasets_df = datasets_df.drop(columns=['users', 'upstreamDatasets'], errors='ignore')\n","        dataflows_df = _clean_columns(pd.DataFrame(all_dataflows_raw)); dataflows_df = dataflows_df.drop(columns=['users'], errors='ignore')\n","\n","        # --- 4. Fetch App and Report Users (if requested) ---\n","        report_users_df, apps_df, app_users_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n","        # (This section remains unchanged)\n","        if include_report_app_users:\n","            logging.info(\"--- Fetching App/Report Users Concurrently ---\")\n","            apps_data = await get_paginated_data_async(client, f\"{admin_base_url}/apps\", headers, params={'$top': 5000})\n","            apps_df = _clean_columns(pd.DataFrame(apps_data))\n","            app_user_tasks = [(get_paginated_data_async(client, f\"{admin_base_url}/apps/{app['id']}/users\", headers), app['id'], app['name']) for index, app in apps_df.iterrows()] if not apps_df.empty else []\n","            report_user_tasks = [(get_paginated_data_async(client, f\"{admin_base_url}/groups/{report['workspace_id']}/reports/{report['id']}/users\", headers), report['workspace_id'], report['id'], report['name']) for index, report in reports_df.iterrows()] if not reports_df.empty else []\n","            all_user_tasks = [t[0] for t in app_user_tasks] + [t[0] for t in report_user_tasks]\n","            all_user_results = await asyncio.gather(*all_user_tasks)\n","            num_app_tasks = len(app_user_tasks)\n","            all_app_users = [{**user, 'app_id': app_id, 'app_name': app_name} for (task, app_id, app_name), users in zip(app_user_tasks, all_user_results[:num_app_tasks]) for user in users]\n","            app_users_df = _normalize_and_merge_json_column(_clean_columns(pd.DataFrame(all_app_users)), 'profile', 'profile_')\n","            all_report_users = [{**user, 'workspace_id': ws_id, 'report_id': report_id, 'report_name': report_name} for (task, ws_id, report_id, report_name), users in zip(report_user_tasks, all_user_results[num_app_tasks:]) for user in users]\n","            report_users_df = _normalize_and_merge_json_column(_clean_columns(pd.DataFrame(all_report_users)), 'profile', 'profile_')\n","\n","        # --- FINAL STEP: Add extraction timestamp to all dataframes ---\n","        all_dfs = {\"capacities\": capacities_df, \"gateways\": gateways_df, \"gateway_datasources\": gateway_datasources_df, \"gateway_datasource_users\": gateway_datasource_users_df, \"workspaces\": workspaces_df, \"workspaces_users\": workspace_users_df, \"reports\": reports_df, \"report_users\": report_users_df, \"datasets\": datasets_df, \"dataflows\": dataflows_df, \"apps\": apps_df, \"app_users\": app_users_df}\n","\n","        # Convert timestamp to an integer (Unix epoch seconds) to match the existing table schema\n","        extraction_ts_int = int(pd.Timestamp.now().timestamp())\n","\n","        for name, df in all_dfs.items():\n","            if not df.empty:\n","                df['extraction_timestamp'] = extraction_ts_int # Assign the integer\n","\n","        return all_dfs\n","\n","async def run_extraction_to_fabric(include_report_app_users: bool = True):\n","    \"\"\"Main async function to run the metadata extraction and save to Fabric Warehouse.\"\"\"\n","    try:\n","        logging.info(\"1. Getting SparkSession...\"); spark_session = globals()['spark'];\n","        logging.info(\"2. Loading configuration...\"); config = load_config(); AUTHORITY, SCOPE, BASE_URL, ADMIN_BASE_URL = get_api_constants(config[\"TENANT_ID\"]);\n","        logging.info(\"3. Authenticating Power BI...\"); access_token = get_access_token(config[\"CLIENT_ID\"], config[\"CLIENT_SECRET\"], AUTHORITY, SCOPE);\n","        logging.info(f\"4. User/App data extraction set to: {'Include' if include_report_app_users else 'Exclude'}\")\n","        logging.info(\"5. Starting concurrent metadata extraction...\")\n","        all_data_dfs = await get_all_metadata_async(access_token, BASE_URL, ADMIN_BASE_URL, config[\"VERIFY_SSL\"], include_report_app_users=include_report_app_users)\n","        logging.info(\"\\n6. Saving extracted metadata to Microsoft Fabric Warehouse...\")\n","        save_to_fabric_warehouse(all_data_dfs, warehouse_schema=\"powerbi_metadata\", spark=spark_session)\n","        logging.info(\"✅ 7. Metadata extraction and saving to Fabric Warehouse completed.\")\n","    except Exception as e:\n","        logging.error(f\"\\n❌ An error occurred during the process: {e}\", exc_info=True)\n","        raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"dffadbfa-4411-41b3-b4bc-74a6250bf888","normalized_state":"finished","queued_time":"2025-07-16T20:25:43.8867736Z","session_start_time":null,"execution_start_time":"2025-07-16T20:26:13.2365923Z","execution_finish_time":"2025-07-16T20:26:13.5125066Z","parent_msg_id":"e1887d35-01da-454c-9ec1-0b98ece754c7"},"text/plain":"StatementMeta(, dffadbfa-4411-41b3-b4bc-74a6250bf888, 38, Finished, Available, Finished)"},"metadata":{}}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9923b52-27b7-4df3-bdb7-ca9b460a951d"},{"cell_type":"markdown","source":["###### **Change include_report_app_users from False to True if you want to include them.**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0369cf8-bcd0-493c-a1b9-eaebfc9256ab"},{"cell_type":"code","source":["# run_extraction_to_fabric(include_report_app_users=False)\n","await run_extraction_to_fabric(include_report_app_users=False)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c3c9ab5d-6338-4ab0-9505-19362819a2c2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"04562fe6-e41b-4e15-80c1-0585600c90d4"}],"default_lakehouse":"04562fe6-e41b-4e15-80c1-0585600c90d4","default_lakehouse_name":"PowerBI_Metadata","default_lakehouse_workspace_id":"c937daa0-0279-4f6d-baa3-652ba7a2c960"}}},"nbformat":4,"nbformat_minor":5}